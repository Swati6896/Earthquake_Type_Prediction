# -*- coding: utf-8 -*-
"""Earthquake_Type_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G-l7ibTJYpGMhQRdhaMfQItBY3C11ERZ

# Earthquake Type Prediction

1. **Research Objective** - From given datasets of Earthquake between 1965- 2016, trying to predict, if a given earthquake is **automatic or reviewed**.
(An earthquake is "reviewed" when automatic earthquake detection systems fail to record the earthquake.)

2. About the dataset - Significant Earthquakes, 1965-2016, sourced from Kaggle [link](https://www.kaggle.com/datasets/usgs/earthquake-database)


We will use a TensorFlow ANN to make our predictions.

##Data Wrangling

### Gathering Data

```
# This is formatted as code
```
"""

#importing libraires

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import tensorflow as tf

df = pd.read_csv("database.csv")

df

df.info()

"""##Pre-Processing"""

df.isna()

df.isna().sum()

# We are dropping those columns which have more than two third of their values are missing.
df.isna().sum() > 0.66 * df.shape[0]

#list of columns which have two third null values

 null_columns = df.loc[:,df.isna().sum() > 0.66 * df.shape[0]].columns

null_columns

#dropping the above columns
df= df.drop(null_columns, axis=1)

#checking again for the columns having missing values
df.isna().sum()

# In root Mean Square column we will replace the missing values with calculated mean value of the whole column

df['Root Mean Square'] = df['Root Mean Square'].fillna(df['Root Mean Square'].mean())

df

df.isna().sum()

#now dropping the rows which have missing values from the dataframe
df = df.dropna(axis=0).reset_index(drop=True)

#Now we can see there ain't any missing values in the dataset

df.isna().sum().sum()

df.drop('ID', axis =1)

"""#Feature Engineering"""

# we are reteriving month and year from the date column and making a seperate column for month and year

df['Month'] = df['Date'].apply(lambda x: x[0:2])
df['Year'] = df['Date'].apply(lambda x: x[-4: ])

# Since we have the Month and Year, We will drop the original date column
df = df.drop('Date', axis=1)

#since we have sliced as a string, therefore we have to convert them into integer (both the columns)
df['Month'] = df['Month'].astype(np.int)
df['Year'] = df['Year'].astype(np.int)

# it looks like that in year column there are inputs like '000Z'. We are finding rows containing such values
df[df['Year'].str.contains('Z')]

#There are only three such rows. Therefore, dropping those rows

df= df.drop(df[df['Year'].str.contains('Z')].index, axis=0).reset_index(drop=True)

df['Month'] = df['Month'].astype(np.int)
df['Year'] = df['Year'].astype(np.int)

# From time column, retrieving hour and type casting into integer
df['Hour'] = df['Time'].apply(lambda x: np.int(x[0:2]))

# dropping time and date column
df = df.drop('Time', axis=1)
df = df.drop('ID', axis=1)

df

df['Status'].unique()

df['Status']=df['Status'].apply(lambda x:1 if x=='Reviewed' else 0)

"""#Visualisation"""

numeric_columns = [column for column in df.columns if df.dtypes[column] != 'object']

#Making correlation matrix
corr = df[numeric_columns].corr()

corr

plt.figure(figsize=(12, 10))
sns.heatmap(corr, annot = True, vmin= -1, vmax= 1)
plt.show()

#removing column status from
numeric_columns.remove('Status')

#Scaling the plot
plt.figure(figsize=(18,10))
for column in numeric_columns:
  sns.kdeplot(df[column], shade=True)
plt.show()

scaler = StandardScaler()
standardized_df = pd.DataFrame(scaler.fit_transform(df[numeric_columns].copy()), columns=numeric_columns)

#again replotting
plt.figure(figsize=(18,10))
for column in numeric_columns:
  sns.kdeplot(standardized_df[column], shade=True)
  plt.xlim(-3,3)
plt.show()

"""#Encoding"""

df

#We used unique to find if there is any binary input. If it's a binary value then we can use lambda functiona and change into 1 and 0.
df['Type'].unique()

"""

```
# Since here we can see that'Type' column is taking more than 2 values, therefore we are using get_dummies function`
```

"""

pd.get_dummies(df['Type'], prefix='TYPE')

def onehot_encode(data, columns, prefixes):
  data = data.copy()
  for column, prefix in zip(columns, prefixes):
    dummies =pd.get_dummies (data[column], prefix=prefix)
    data = pd.concat([data,dummies], axis=1)
    data = data.drop(column, axis=1)
  return data

df = onehot_encode(df, ['Type','Magnitude Type','Source', 'Location Source', 'Magnitude Source'], ['t','mt','s','ls','ms'])

df

"""# Splitting and Scaling"""

#splitting into x and y
y = df.loc[:,'Status']
X = df.drop('Status', axis=1)

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=56)

"""#Modelling and Training

"""

X.shape

y

y.mean()

inputs = tf.keras. Input (shape=(104,))
x = tf.keras.layers.Dense (64, activation='relu') (inputs)
x = tf.keras.layers. Dense (64, activation= 'relu')(x)
outputs = tf.keras.layers.Dense (1, activation= 'sigmoid')(x)

model = tf.keras. Model (inputs, outputs)
model.compile(
  optimizer='adam',
  loss='binary_crossentropy',
  metrics=[tf.keras.metrics.AUC (name='auc')]
)
batch_size = 32
epochs = 30

history= model.fit(
  X_train,
  y_train, batch_size-batch_size,
  validation_split=0.2,
  epochs= epochs,
  callbacks=[tf.keras.callbacks. ReduceLROnPlateau()]
)

verbose=0

"""#Results"""

plt.figure(figsize=(18, 6))

epochs_range = range(epochs)
train_loss, val_loss = history.history['loss' ], history.history['val_loss']
train_auc, val_auc = history.history['auc'], history.history['val_auc']

plt.subplot(1, 2, 1)
plt.plot(epochs_range, train_loss, label="Training Loss")
plt.plot(epochs_range, val_loss, label="Validation Loss")
plt.legend()
plt.title("Loss Over Time")

plt.subplot(1, 2, 2)
plt.plot(epochs_range, train_auc, label="Training AUC")
plt.plot(epochs_range, val_auc, label="Validation AUC")
plt.legend()
plt.title("AUC Over Time")

plt.show()

model.evaluate(X_test, y_test)

len(y_test)